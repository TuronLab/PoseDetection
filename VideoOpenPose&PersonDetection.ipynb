{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the model to be used\n",
    "COCO and MPI are body pose estimation model. COCO has 18 points and MPI has 15 points as output.\n",
    "\n",
    "HAND is hand keypoints estimation model. It has 22 points as output\n",
    "\n",
    "Ensure that the model files are available in the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"MPI\"\n",
    "\n",
    "if MODE is \"COCO\":\n",
    "    protoFile = \"Models/coco/pose_deploy_linevec.prototxt\"\n",
    "    weightsFile = \"Models/coco/pose_iter_440000.caffemodel\"\n",
    "    nPoints = 18\n",
    "    POSE_PAIRS = [ [1,0],[1,2],[1,5],[2,3],[3,4],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13],[0,14],[0,15],[14,16],[15,17]]\n",
    "\n",
    "elif MODE is \"MPI\" :\n",
    "    protoFile = \"Models/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "    weightsFile = \"Models/mpi/pose_iter_160000.caffemodel\"\n",
    "    nPoints = 15\n",
    "    POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COCO Output Format</b> Nose – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4, Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8, Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12, LAnkle – 13, Right Eye – 14, Left Eye – 15, Right Ear – 16, Left Ear – 17, Background – 18\n",
    "\n",
    "<b>MPII Output Format</b> Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4, Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8, Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12, Left Ankle – 13, Chest – 14, Background – 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the network and set the input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skNet = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "inWidth = 368\n",
    "inHeight = 368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPoints(frame, output, W, H, frameWidth, frameHeight, sPoint, threshold):\n",
    "    # Plots bodypoints and relations detected\n",
    "\n",
    "    # Empty list to store the detected keypoints\n",
    "    points = []\n",
    "\n",
    "    for i in range(nPoints):\n",
    "        # confidence map of corresponding body's part.\n",
    "        probMap = output[0, i, :, :]\n",
    "\n",
    "        # Find global maxima of the probMap.\n",
    "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "        # Scale the point to fit on the original image\n",
    "        x = (frameWidth * point[0]) / W\n",
    "        y = (frameHeight * point[1]) / H\n",
    "\n",
    "        if prob > threshold :\n",
    "            cv2.circle(frame, (int(x), int(y)), 8*sPoint, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "            # Add the point to the list if the probability is greater than the threshold\n",
    "            points.append((int(x), int(y)))\n",
    "        else :\n",
    "            points.append(None)\n",
    "\n",
    "    for pair in POSE_PAIRS:\n",
    "        partA = pair[0]\n",
    "        partB = pair[1]\n",
    "\n",
    "        if points[partA] and points[partB]:\n",
    "            cv2.line(frame, points[partA], points[partB], (0, 255, 255), 3*sPoint)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoxes(outs):\n",
    "     # Get specs of the box where an object has been detected\n",
    "        \n",
    "    class_ids = [] # Indicates the type of object that has been found (0: person)\n",
    "    confidences = [] # The score that each class has had\n",
    "    boxes = [] # Boxes coordinates\n",
    "    \n",
    "    #create bounding box \n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.1:\n",
    "                center_x = int(detection[0] * frameWidth)\n",
    "                center_y = int(detection[1] * frameHeight)\n",
    "                w = int(detection[2] * frameWidth)\n",
    "                h = int(detection[3] * frameHeight)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h]) #(xUpLeft, yUpLeft, widthObject, heigthObject)\n",
    "    return class_ids, confidences, boxes\n",
    "    \n",
    "def getPeople(indices, class_ids, confidences, boxes, showBox = True):\n",
    "    # Plots rectangle of every person detected if showBox = True, and\n",
    "    # returns the area of each person detected\n",
    "    \n",
    "    area = [] # Rectangle Area\n",
    "    personBoxes = []\n",
    "\n",
    "    # Check if is people detection\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        if class_ids[i]==0:\n",
    "            area.append(box[2]*box[3]) # width*height\n",
    "            personBoxes.append(box)\n",
    "            label = str(classes[class_ids[i]])\n",
    "            if showBox:\n",
    "                cv2.rectangle(frame, (round(box[0]),round(box[1])), (round(box[0]+box[2]),round(box[1]+box[3])), (0, 0, 255), 2)\n",
    "                cv2.putText(frame, label, (round(box[0])-10,round(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    return area, personBoxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo & Sk Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "path = \"Models\\\\yolov3\\\\\"\n",
    "\n",
    "classes = None\n",
    "with open(path+'coco.names', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "yoloNet = cv2.dnn.readNet(path+'yolov3-spp.weights', path+'yolov3-spp.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model? 0: no, Other: Name of file. 1_1BlurrBoth\n",
      "Calculate skeleton with blurred frames? 1. Yes, Other. No 1\n"
     ]
    }
   ],
   "source": [
    "name = \"VideoSkier\"\n",
    "\n",
    "cap = cv2.VideoCapture(\"Dataset\\\\\"+name+\".mp4\")\n",
    "\n",
    "#Para hacerlo a cámara lenta\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)/2 # /2 for slowmo\n",
    "\n",
    "if (cap.isOpened()== False):\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "frame = []\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "frameWidth = frame.shape[1]\n",
    "frameHeight = frame.shape[0]\n",
    "\n",
    "# Confidence margin to cut biggest person detected\n",
    "mConf = 0.3\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "\n",
    "save = input(\"Save model? 0: no, Other: Name of file. \")\n",
    "while type(save) != str: save = input(\"Insert a valid name \")\n",
    "\n",
    "if save != \"0\":\n",
    "    out = cv2.VideoWriter('Output\\\\'+save+'.mp4', fourcc, fps, (frameWidth, frameHeight))\n",
    "    \n",
    "    \n",
    "blurr = input(\"Calculate skeleton with blurred frames? 1. Yes, Other. No \")\n",
    "while type(blurr) != str: blurr = input(\"Insert a valid option \")\n",
    "\n",
    "sPoint = 1\n",
    "numFrame = 0\n",
    "\n",
    "farObject = True\n",
    "threshold = 0.1\n",
    "thresArea = 50\n",
    "thrs2Yolo = 0.04\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "\n",
    "        ###############################YOLO PERSON DETECTION############################\n",
    "        \n",
    "        if numFrame %fps == 0 or farObject:\n",
    "            yoloNet.setInput(cv2.dnn.blobFromImage(frame, 0.00392, (416,416), (0,0,0), True, crop=False))\n",
    "\n",
    "            layer_names = yoloNet.getLayerNames()\n",
    "            output_layers = [layer_names[i[0] - 1] for i in yoloNet.getUnconnectedOutLayers()]\n",
    "            outs = yoloNet.forward(output_layers)\n",
    "\n",
    "            class_ids, confidences, boxes = getBoxes(outs)\n",
    "\n",
    "            indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.1, 0.1)\n",
    "\n",
    "            area, personBoxes = getPeople(indices, class_ids, confidences, boxes, showBox = False)\n",
    "            \n",
    "            if area != [] and thrs2Yolo > max(area)/(frameWidth*frameHeight) and max(area) > thresArea:\n",
    "                farObject = True\n",
    "            else:\n",
    "                farObject = False\n",
    "        \n",
    "        ###############################SKELETON DETECTION#############################\n",
    "        \n",
    "        # Cut Biggest Person\n",
    "\n",
    "        if farObject: # If person detected\n",
    "            idx = area.index(max(area))\n",
    "            box = personBoxes[idx]\n",
    "            \n",
    "            # Calculate position of Biggest Person + Security Range\n",
    "            posBiggest = [max(floor(box[0]-(mConf*box[2])),0), min(floor((box[0]+box[2])+(mConf*box[2])), frameWidth-1),\n",
    "                        max(floor(box[1]-(mConf*box[3])),0), min(floor((box[1]+box[3])+(mConf*box[3])), frameHeight-1)]\n",
    "\n",
    "            # Resize image until input features for Sk detection are satisfied\n",
    "            while posBiggest[1]-posBiggest[0] < inWidth:\n",
    "                posBiggest[0] = max(posBiggest[0]-1,0)\n",
    "                posBiggest[1] = min(posBiggest[1]+1,frameWidth-1)\n",
    "            while posBiggest[3]-posBiggest[2] < inHeight:\n",
    "                posBiggest[2] = max(posBiggest[2]-1,0)\n",
    "                posBiggest[3] = min(posBiggest[3]+1,frameHeight-1)\n",
    "        \n",
    "        if farObject: # If person detected, cut the frame to see only that person\n",
    "            \n",
    "            person = frame[posBiggest[2]:posBiggest[3],posBiggest[0]:posBiggest[1]]\n",
    "            \n",
    "            if blurr == \"1\": # If blurred frames option is activated\n",
    "                inpBlob = cv2.dnn.blobFromImage(cv2.GaussianBlur(deepcopy(person),(5,5), sigmaX = 0, sigmaY = 1), \n",
    "                                                1.0 / 255, (person.shape[1], person.shape[0]),\n",
    "                                                (0, 0, 0), swapRB=False, crop=False)\n",
    "            else:\n",
    "                inpBlob = cv2.dnn.blobFromImage(person, 1.0 / 255, (person.shape[1], person.shape[0]), \n",
    "                                                (0, 0, 0), swapRB=False, crop=False)\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            if blurr == \"1\": # If blurred frames option is activated\n",
    "                inpBlob = cv2.dnn.blobFromImage(cv2.GaussianBlur(deepcopy(frame),(5,5), sigmaX = 0, sigmaY = 1), \n",
    "                                                1.0 / 255, (inWidth, inHeight),\n",
    "                                                (0, 0, 0), swapRB=False, crop=False)\n",
    "            else:\n",
    "                inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), \n",
    "                                                (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "        skNet.setInput(inpBlob)\n",
    "\n",
    "        output = skNet.forward()\n",
    "        H = output.shape[2]\n",
    "        W = output.shape[3]\n",
    "        \n",
    "        if farObject:\n",
    "            person = showPoints(person, output, W, H, person.shape[1], \n",
    "                               person.shape[0], sPoint, threshold)\n",
    "            frame[posBiggest[2]:posBiggest[3],posBiggest[0]:posBiggest[1]] = person\n",
    "        else:\n",
    "            frame = showPoints(frame, output, W, H, frameWidth, frameHeight, sPoint, threshold)\n",
    "\n",
    "        if save != \"0\": out.write(frame.astype('uint8'))\n",
    "        cv2.imshow('Frame', frame.astype('uint8'))\n",
    "        \n",
    "        numFrame += 1\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "if save != \"0\": out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
